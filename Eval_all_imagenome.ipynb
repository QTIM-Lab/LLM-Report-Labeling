{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c46b4-775e-40db-964d-429d2be236d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a8636",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframes(dataframes):\n",
    "    for df_name, df in dataframes.items():\n",
    "        # First, replace specific values, to make them all conform to one standard\n",
    "        df.replace({\"yes\": \"Yes\", \"no\": \"No\", \"None\": \"No\"}, inplace=True)\n",
    "        df.replace({1: \"Yes\", \"0\": \"No\", -1: \"Maybe\", \"None\": \"No\"}, inplace=True)\n",
    "        df.replace({1.0: \"Yes\", 0.0: \"No\", -1.0: \"Maybe\", np.nan: \"No\"}, inplace=True)\n",
    "        df.replace({\"1.0\": \"Yes\", \"0.0\": \"No\", \"-1.0\": \"Maybe\", \"nan\": \"No\"}, inplace=True)\n",
    "        df.replace({\"Maybe\": \"Yes\"}, inplace=True)\n",
    "\n",
    "        \n",
    "\n",
    "        # Then, replace all values that are not \"Yes\" with \"No\"\n",
    "        dataframes[df_name] = dataframes[df_name].apply(lambda col: col.map(lambda x: \"No\" if x != \"Yes\" else \"Yes\"))\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(df_ground_truth, df_predictions):\n",
    "    \"\"\"\n",
    "    Calculate individual, average, micro, and macro F1 scores for multi-label classification.\n",
    "\n",
    "    Parameters:\n",
    "    - df_ground_truth: DataFrame containing the ground truth labels.\n",
    "    - df_predictions: DataFrame containing the predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the dictionary of F1 scores for each label, average F1 score, micro F1 score, and macro F1 score.\n",
    "    \"\"\"\n",
    "    # Check for values other than 'Yes' or 'No' and notify replacement\n",
    "    for df in [df_ground_truth, df_predictions]:\n",
    "        if df.isin(['Yes', 'No']).all().all() == False:\n",
    "            raise ValueError(f\"{df} dataframe contains values other than 'Yes' or 'No'. Please ensure all values are 'Yes' or 'No'.\")\n",
    "\n",
    "    # Convert 'Yes'/'No' to binary format\n",
    "    df_ground_truth_binary = df_ground_truth.apply(lambda col: col.map({'Yes': 1, 'No': 0}))\n",
    "    df_predictions_binary = df_predictions.apply(lambda col: col.map({'Yes': 1, 'No': 0}))\n",
    "\n",
    "    # Align columns\n",
    "    common_columns = df_ground_truth.columns.intersection(df_predictions.columns)\n",
    "    df_ground_truth_binary = df_ground_truth_binary[common_columns]\n",
    "    df_predictions_binary = df_predictions_binary.reindex(columns=df_ground_truth_binary.columns)\n",
    "\n",
    "    # Calculate F1 scores for each label\n",
    "    f1_scores = {label: f1_score(df_ground_truth_binary[label], df_predictions_binary[label])\n",
    "                 for label in common_columns}\n",
    "\n",
    "    # Calculate average, micro, and macro F1 scores\n",
    "    average_f1 = sum(f1_scores.values()) / len(f1_scores)\n",
    "    micro_f1 = f1_score(df_ground_truth_binary.values.ravel(), df_predictions_binary.values.ravel(), average='micro')\n",
    "    macro_f1 = f1_score(df_ground_truth_binary, df_predictions_binary, average='macro', zero_division=0)\n",
    "\n",
    "    return f1_scores, average_f1, micro_f1, macro_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0b34f",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/base/path/for/inference_results/'\n",
    "few_shot_df = pd.read_csv('/base/path/for/inference_results/few_shot_reports_imagenome.csv')\n",
    "few_shot_list = few_shot_df['acc'].tolist()\n",
    "\n",
    "diseases = ['Atelectasis', 'Pleural_Effusion', 'Pneumonia', 'Pneumothorax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8902b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    'gpt_4': os.path.join(base_path, 'zero_shot_gpt_4/gpt_labeled_reports.csv'),\n",
    "    'gpt_35': os.path.join(base_path, 'zero_shot_gpt_35/gpt_labeled_reports.csv'),\n",
    "    'llama13b': os.path.join(base_path, 'zero_shot_llama13b/llm_labeled_reports.csv'),\n",
    "    'llama70b': os.path.join(base_path, 'zero_shot_llama70b/llm_labeled_reports.csv'),\n",
    "    'mistral7b': os.path.join(base_path, 'zero_shot_mistral7b/llm_labeled_reports.csv'),\n",
    "    'mixtral8x7b': os.path.join(base_path, 'zero_shot_mistral7b/llm_labeled_reports.csv'),\n",
    "    'qwen72b': os.path.join(base_path, 'zero_shot_qwen72b/llm_labeled_reports.csv'),\n",
    "\n",
    "    \n",
    "    'ground_truth': \"/base/path/for/inference_results/reports_imagenome_labeled_wo_few_shots.csv\",\n",
    "    'chexbert': \"/base/path/for/inference_results/chexbert_labeled_imagenome.csv\",\n",
    "    'chexpert': \"/base/path/for/inference_results/chexPert_labeled_imagenome.csv\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc29db3",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83077fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {name: utils.load_and_preprocess_data(path, few_shot_list) for name, path in file_paths.items()}\n",
    "\n",
    "\n",
    "#create raw copy that will not get processed for further analysis\n",
    "dataframes_raw = dataframes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dataframes and drop all columns that are not in diseases\n",
    "for name, df in dataframes.items():\n",
    "    for col in df.columns:\n",
    "        if col not in diseases:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    df.replace([\"No Information\", \"Undefined\"], \"No\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = preprocess_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4 = dataframes['gpt_4']\n",
    "gpt_35 = dataframes['gpt_35']\n",
    "\n",
    "llama13b = dataframes['llama13b']\n",
    "llama70b = dataframes['llama70b']\n",
    "mistral7b = dataframes['mistral7b']\n",
    "mixtral8x7b = dataframes['mixtral8x7b']\n",
    "qwen72b = dataframes['qwen72b']\n",
    "ground_truth = dataframes['ground_truth']\n",
    "chexbert = dataframes['chexbert']\n",
    "chexpert = dataframes['chexpert']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine Mixtral, Llama70 and qwen in an ensemble\n",
    "# NOTE: this only works for the binary use case, there is not a straighforward way\n",
    "# to apply this to the multi-class case using just three models (potentially each model has a different label)\n",
    "llama70b_numerical = pd.DataFrame(index=llama70b.index, columns=llama70b.columns)\n",
    "mixtral8x7b_numerical = pd.DataFrame(index=mixtral8x7b.index, columns=mixtral8x7b.columns)\n",
    "qwen_72b_numerical = pd.DataFrame(index=qwen72b.index, columns=qwen72b.columns)\n",
    "\n",
    "\n",
    "llama70b_numerical = llama70b.apply(lambda x: x.map({'Yes': 1, 'No': 0}))\n",
    "mixtral8x7b_numerical = mixtral8x7b.apply(lambda x: x.map({'Yes': 1, 'No': 0}))\n",
    "qwen_72b_numerical = qwen72b.apply(lambda x: x.map({'Yes': 1, 'No': 0}))\n",
    "\n",
    "majority_vote_sum = llama70b_numerical + mixtral8x7b_numerical + qwen_72b_numerical\n",
    "\n",
    "\n",
    "model_ensemble = majority_vote_sum.apply(lambda col: col.map(lambda x: 'Yes' if x >= 2 else 'No'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66de90",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth labels\n",
    "y_true = ground_truth\n",
    "\n",
    "# Prediction DataFrames\n",
    "predictions = {\n",
    "    'gpt_4': gpt_4,\n",
    "    'gpt_35': gpt_35,\n",
    "    'llama13b': llama13b,\n",
    "    'llama70b': llama70b,\n",
    "    'mistral7b': mistral7b,\n",
    "    'mixtral8x7b': mixtral8x7b,\n",
    "    'qwen72b': qwen72b,\n",
    "    'chexbert': chexbert,\n",
    "    'chexpert': chexpert,\n",
    "    'ensemble': model_ensemble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating F1 scores for each model\n",
    "model_scores = {model_name: calculate_f1_scores(y_true, y_pred) for model_name, y_pred in predictions.items()}\n",
    "\n",
    "# Creating DataFrame for model scores\n",
    "scores_data = {}\n",
    "for model_name, (f1_scores, average_f1, micro_f1, macro_f1) in model_scores.items():\n",
    "    for finding, score in f1_scores.items():\n",
    "        scores_data.setdefault(finding, {}).update({f'F1 Score {model_name.title()}': score})\n",
    "    scores_data.setdefault('Average', {}).update({f'F1 Score {model_name.title()}': average_f1})\n",
    "    scores_data.setdefault('Micro F1', {}).update({f'F1 Score {model_name.title()}': micro_f1})\n",
    "    scores_data.setdefault('Macro F1', {}).update({f'F1 Score {model_name.title()}': macro_f1})\n",
    "\n",
    "f1_scores_df = pd.DataFrame(scores_data).T.round(3)\n",
    "\n",
    "# change the order of the columns \n",
    "f1_scores_df = f1_scores_df[['F1 Score Chexpert', 'F1 Score Chexbert', 'F1 Score Mistral7B',  'F1 Score Llama13B',  'F1 Score Mixtral8X7B', 'F1 Score Llama70B','F1 Score Qwen72B', 'F1 Score Ensemble', 'F1 Score Gpt_35', 'F1 Score Gpt_4']]\n",
    "f1_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe788d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inference results\n",
    "f1_scores_df.to_csv(os.path.join(base_path, 'f1_scores_few_shot_positive.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c143e",
   "metadata": {},
   "source": [
    "## Label Frequency in Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count values for each column\n",
    "ground_truth_count = ground_truth.apply(pd.Series.value_counts)\n",
    "ground_truth_count = ground_truth_count.T.fillna(0).astype(int)\n",
    "\n",
    "#ground_truth_count.to_csv('ground_truth_count.csv')\n",
    "ground_truth_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a1bc1",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test_corrected(model1, model2, diseases, ground_truth):\n",
    "    valid_values = {\"Yes\", \"No\"}\n",
    "    \n",
    "    def check_values(df, columns):\n",
    "        for col in columns:\n",
    "            if not df[col].isin(valid_values).all():\n",
    "                raise ValueError(f\"Column {col} contains values other than 'Yes' or 'No'.\")\n",
    "    \n",
    "    check_values(model1, diseases)\n",
    "    check_values(model2, diseases)\n",
    "\n",
    "    results = []\n",
    "    combined_contingency = [0, 0, 0, 0]  # yes_yes, yes_no, no_yes, no_no\n",
    "\n",
    "    for disease in diseases:\n",
    "        correct_correct = ((model1[disease] == ground_truth[disease]) & (model2[disease] == ground_truth[disease])).sum()\n",
    "        correct_incorrect = ((model1[disease] == ground_truth[disease]) & (model2[disease] != ground_truth[disease])).sum()\n",
    "        incorrect_correct = ((model1[disease] != ground_truth[disease]) & (model2[disease] == ground_truth[disease])).sum()\n",
    "        incorrect_incorrect = ((model1[disease] != ground_truth[disease]) & (model2[disease] != ground_truth[disease])).sum()\n",
    "        \n",
    "        combined_contingency[0] += correct_correct\n",
    "        combined_contingency[1] += correct_incorrect\n",
    "        combined_contingency[2] += incorrect_correct\n",
    "        combined_contingency[3] += incorrect_incorrect\n",
    "        \n",
    "        contingency_table = [[correct_correct, correct_incorrect], [incorrect_correct, incorrect_incorrect]]\n",
    "\n",
    "        result = mcnemar(contingency_table, exact=True if min(correct_incorrect, incorrect_correct) < 25 else False)\n",
    "        stat = result.statistic\n",
    "        p = result.pvalue\n",
    "        \n",
    "        results.append({\n",
    "            \"Disease\": disease,\n",
    "            \"McNemar Statistic\": stat,\n",
    "            \"P-value\": p\n",
    "        })\n",
    "    \n",
    "    # Perform McNemar test on the combined contingency table\n",
    "    combined_table = [[combined_contingency[0], combined_contingency[1]],\n",
    "                      [combined_contingency[2], combined_contingency[3]]]\n",
    "\n",
    "    combined_result = mcnemar(combined_table, exact=True if np.sum([combined_contingency[1], combined_contingency[2]])<25 else False)\n",
    "    # Append combined results\n",
    "    results.append({\n",
    "        \"Disease\": \"All Combined\",\n",
    "        \"McNemar Statistic\": combined_result.statistic,\n",
    "        \"P-value\": combined_result.pvalue\n",
    "    })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    p_values = results_df[\"P-value\"].values\n",
    "    bonferroni_corrected = multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "    results_df['Bonferroni-corrected P-value'] = bonferroni_corrected[1]\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "aggregated_results_mcnemar = pd.DataFrame()\n",
    "\n",
    "for model_name, model_predictions in predictions.items():\n",
    "\n",
    "\n",
    "    results_df_mcnemar = mcnemar_test_corrected(gpt_4, model_predictions, diseases, y_true)\n",
    "    results_df_mcnemar['Model'] = model_name  # Add a column for the model name\n",
    "    # Ensure the 'Disease' column is included before concatenation\n",
    "    aggregated_results_mcnemar = pd.concat([aggregated_results_mcnemar, results_df_mcnemar], axis=0)\n",
    "\n",
    "\n",
    "# Resetting index for better readability and to avoid duplicate indices\n",
    "aggregated_results_mcnemar.reset_index(drop=True, inplace=True)\n",
    "aggregated_results_mcnemar.set_index([\"Model\"], inplace=True)\n",
    "total_mcnemar = aggregated_results_mcnemar[aggregated_results_mcnemar[\"Disease\"] == \"All Combined\"]\n",
    "total_mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test results to csv\n",
    "total_mcnemar.to_csv(os.path.join(base_path, 'total_mcnemar_zero_shot.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray-drift-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c46b4-775e-40db-964d-429d2be236d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes sure that the font is recognized in Adobe Illustrator as Text\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a8636",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframes(dataframes, binary=False):\n",
    "    for df_name, df in dataframes.items():\n",
    "        # First, replace specific values, to make them all conform to one standard\n",
    "        df.replace({\"yes\": \"Yes\", \"no\": \"No\", \"None\": \"No\"}, inplace=True)\n",
    "        df.replace({np.NaN: \"No\"}, inplace=True)\n",
    "        df.replace({1: \"Yes\", \"0\": \"No\", -1: \"Maybe\", \"maybe\": \"Maybe\", \"1\": \"Yes\", \"1.0\":\"Yes\"}, inplace=True)\n",
    "\n",
    "        lung_opacity_conditions = ['Edema', 'Consolidation', 'Pneumonia', 'Lung_Lesion', 'Atelectasis']\n",
    "        cardiomegaly_conditions = ['Cardiomegaly']\n",
    "\n",
    "        utils.update_column(df, 'Lung_Opacity', lung_opacity_conditions)\n",
    "        utils.update_column(df, 'Enlarged_Cardiomediastinum', cardiomegaly_conditions)\n",
    "\n",
    "\n",
    "        # Then, replace all values that are not Yes or Maybe with No\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].apply(lambda x: \"Yes\" if x == \"Yes\" else (\"Maybe\" if x == \"Maybe\" else \"No\"))\n",
    "\n",
    "        # replace maybe with yes\n",
    "        if binary:\n",
    "            df.replace({\"Maybe\": \"Yes\"}, inplace=True)\n",
    "        \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(df_ground_truth, df_predictions):\n",
    "    \"\"\"\n",
    "    Calculate individual, average, micro, and macro F1 scores for multi-label classification.\n",
    "\n",
    "    Parameters:\n",
    "    - df_ground_truth: DataFrame containing the ground truth labels.\n",
    "    - df_predictions: DataFrame containing the predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the dictionary of F1 scores for each label, average F1 score, micro F1 score, and macro F1 score.\n",
    "    \"\"\"\n",
    "    # Check for values other than 'Yes' or 'No' and notify replacement\n",
    "    for df in [df_ground_truth, df_predictions]:\n",
    "        if df.isin(['Yes', 'No']).all().all() == False:\n",
    "            warnings.warn(\"dataframe contains values other than 'Yes' or 'No'. Please ensure all values are 'Yes' or 'No'.\", UserWarning)\n",
    "\n",
    "    # Convert 'Yes'/'No' to binary format\n",
    "    df_ground_truth_binary = df_ground_truth.apply(lambda col: col.map({'Yes': 1, 'No': 0}))\n",
    "    df_predictions_binary = df_predictions.apply(lambda col: col.map({'Yes': 1, 'No': 0}))\n",
    "\n",
    "    # Align columns\n",
    "    common_columns = df_ground_truth.columns.intersection(df_predictions.columns)\n",
    "    df_ground_truth_binary = df_ground_truth_binary[common_columns]\n",
    "    df_predictions_binary = df_predictions_binary.reindex(columns=df_ground_truth_binary.columns)\n",
    "\n",
    "    # Calculate F1 scores for each label\n",
    "    f1_scores = {label: f1_score(df_ground_truth_binary[label], df_predictions_binary[label])\n",
    "                 for label in common_columns}\n",
    "\n",
    "    # Calculate average, micro, and macro F1 scores\n",
    "    average_f1 = sum(f1_scores.values()) / len(f1_scores)\n",
    "    micro_f1 = f1_score(df_ground_truth_binary.values.ravel(), df_predictions_binary.values.ravel(), average='micro')\n",
    "    macro_f1 = f1_score(df_ground_truth_binary, df_predictions_binary, average='macro', zero_division=0)\n",
    "\n",
    "    return f1_scores, average_f1, micro_f1, macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cohens_kappa(df_ground_truth, df_predictions):\n",
    "    # Map 'Yes', 'Maybe', 'No' to numeric values\n",
    "    class_mapping = {'Yes': 1, 'Maybe': 2, 'No': 0}\n",
    "    df_ground_truth_mapped = df_ground_truth.apply(lambda col: col.map(class_mapping))\n",
    "    df_predictions_mapped = df_predictions.apply(lambda col: col.map(class_mapping))\n",
    "\n",
    "    # Align columns\n",
    "    common_columns = df_ground_truth.columns.intersection(df_predictions.columns)\n",
    "    df_ground_truth_mapped = df_ground_truth_mapped[common_columns]\n",
    "    df_predictions_mapped = df_predictions_mapped.reindex(columns=df_ground_truth_mapped.columns)\n",
    "\n",
    "    # Calculate Cohen's Kappa for each finding\n",
    "    cohens_kappa_scores = {finding: cohen_kappa_score(df_ground_truth_mapped[finding], df_predictions_mapped[finding])\n",
    "                           for finding in df_ground_truth_mapped.columns}\n",
    "\n",
    "\n",
    "    average_kappa = sum(cohens_kappa_scores.values()) / len(cohens_kappa_scores)\n",
    "\n",
    "    return cohens_kappa_scores, average_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b49987",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b54c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/base/path/for/inference_results/'\n",
    "\n",
    "diseases = ['Atelectasis',\n",
    "'Cardiomegaly',\n",
    "'Consolidation',\n",
    "'Edema',\n",
    "'Lung_Lesion',\n",
    "'Lung_Opacity',\n",
    "'Pleural_Other',\n",
    "'Pleural_Effusion',\n",
    "'Pneumonia',\n",
    "'Pneumothorax',\n",
    "'Support_Devices',\n",
    "'Enlarged_Cardiomediastinum',\n",
    "'Fracture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    'gpt_4': os.path.join(base_path, 'few_shot_positive_gpt_4/gpt_labeled_reports.csv'),\n",
    "    'gpt_35': os.path.join(base_path, 'few_shot_positive_gpt_35/gpt_labeled_reports.csv'),\n",
    "    'llama13b': os.path.join(base_path, 'few_shot_positive_llama13b/llm_labeled_reports.csv'),\n",
    "    'llama70b': os.path.join(base_path, 'few_shot_positive_llama70b/llm_labeled_reports.csv'),\n",
    "    'mistral7b': os.path.join(base_path, 'few_shot_positive_mistral7b/llm_labeled_reports.csv'),\n",
    "    'mixtral8x7b': os.path.join(base_path, 'few_shot_positive_mixtral8x7b/llm_labeled_reports.csv'),\n",
    "    'qwen_72b': os.path.join(base_path, 'few_shot_positive_qwen72b/llm_labeled_reports.csv'),\n",
    "    \n",
    "    'ground_truth': \"/base/path/for/inference_results/groundtruth.csv\",\n",
    "    'chexbert': \"/base/path/for/inference_results/chexbert_labeled_mgb.csv\",\n",
    "    'chexpert': \"/base/path/for/inference_results/chexpert_labeled_reports_mgb.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e85226",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {name: utils.load_and_preprocess_data(path) for name, path in file_paths.items()}\n",
    "\n",
    "#create raw copy that will not get processed for further analysis\n",
    "dataframes_raw = dataframes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dataframes and drop all columns that are not in diseases\n",
    "for name, df in dataframes.items():\n",
    "    for col in df.columns:\n",
    "        if col not in diseases:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    df.replace([\"No Information\", \"Undefined\"], \"No\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9307eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using \"Maybe\" as a seperate class set binary=False\n",
    "dataframes = preprocess_dataframes(dataframes, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db798c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4 = dataframes['gpt_4']\n",
    "gpt_35 = dataframes['gpt_35']\n",
    "\n",
    "llama13b = dataframes['llama13b']\n",
    "llama70b = dataframes['llama70b']\n",
    "mistral7b = dataframes['mistral7b']\n",
    "mixtral8x7b = dataframes['mixtral8x7b']\n",
    "qwen_72b = dataframes['qwen_72b']\n",
    "\n",
    "ground_truth = dataframes['ground_truth']\n",
    "chexbert = dataframes['chexbert']\n",
    "chexpert = dataframes['chexpert']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa542088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine Mixtral, Llama70 and qwen in an ensemble\n",
    "# NOTE: this only works for the binary use case, there is not a straighforward way\n",
    "# to apply this to the multi-class case using just three models (potentially each model has a different label)\n",
    "llama70b_numerical = pd.DataFrame(index=llama70b.index, columns=llama70b.columns)\n",
    "mixtral8x7b_numerical = pd.DataFrame(index=mixtral8x7b.index, columns=mixtral8x7b.columns)\n",
    "qwen_72b_numerical = pd.DataFrame(index=qwen_72b.index, columns=qwen_72b.columns)\n",
    "\n",
    "\n",
    "llama70b_numerical = llama70b.apply(lambda x: x.map({'Yes': 1, 'No': 0}))\n",
    "mixtral8x7b_numerical = mixtral8x7b.apply(lambda x: x.map({'Yes': 1, 'No': 0}))\n",
    "qwen_72b_numerical = qwen_72b.apply(lambda x: x.map({'Yes': 1, 'No': 0}))\n",
    "\n",
    "majority_vote_sum = llama70b_numerical + mixtral8x7b_numerical + qwen_72b_numerical\n",
    "\n",
    "\n",
    "model_ensemble = majority_vote_sum.apply(lambda col: col.map(lambda x: 'Yes' if x >= 2 else 'No'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66de90",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth labels\n",
    "y_true = ground_truth\n",
    "\n",
    "# Prediction DataFrames\n",
    "predictions = {\n",
    "    'gpt_4': gpt_4,\n",
    "    'gpt_35': gpt_35,\n",
    "    'llama13b': llama13b,\n",
    "    'llama70b': llama70b,\n",
    "    'mistral7b': mistral7b,\n",
    "    'mixtral8x7b': mixtral8x7b,\n",
    "    'mixtral8x7b': mixtral8x7b,\n",
    "    'qwen_72b': qwen_72b,\n",
    "    'chexbert': chexbert,\n",
    "    'chexpert': chexpert,\n",
    "    'ensemble': model_ensemble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fda10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating F1 scores for each model\n",
    "model_scores = {model_name: calculate_f1_scores(y_true, y_pred) for model_name, y_pred in predictions.items()}\n",
    "\n",
    "# Creating DataFrame for model scores\n",
    "scores_data = {}\n",
    "for model_name, (f1_scores, average_f1, micro_f1, macro_f1) in model_scores.items():\n",
    "    for finding, score in f1_scores.items():\n",
    "        scores_data.setdefault(finding, {}).update({f'F1 Score {model_name.title()}': score})\n",
    "    scores_data.setdefault('Average', {}).update({f'F1 Score {model_name.title()}': average_f1})\n",
    "    scores_data.setdefault('Micro F1', {}).update({f'F1 Score {model_name.title()}': micro_f1})\n",
    "    scores_data.setdefault('Macro F1', {}).update({f'F1 Score {model_name.title()}': macro_f1})\n",
    "\n",
    "\n",
    "f1_scores_df = pd.DataFrame(scores_data).T.round(3)\n",
    "f1_scores_df = f1_scores_df[['F1 Score Chexpert', 'F1 Score Chexbert', 'F1 Score Mistral7B',  'F1 Score Llama13B',  'F1 Score Mixtral8X7B', 'F1 Score Llama70B','F1 Score Qwen_72B', 'F1 Score Ensemble', 'F1 Score Gpt_35', 'F1 Score Gpt_4']]\n",
    "f1_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5407b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inference results\n",
    "f1_scores_df.to_csv(os.path.join(base_path, 'f1_scores_zero_shot_binary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e47d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kappa_scores = {model_name: calculate_cohens_kappa(y_true, y_pred) for model_name, y_pred in predictions.items()}\n",
    "\n",
    "# Creating DataFrame for Cohen's Kappa scores\n",
    "kappa_scores_data = {}\n",
    "for model_name, (cohens_kappa_scores, average_kappa) in model_kappa_scores.items():\n",
    "    for finding, score in cohens_kappa_scores.items():\n",
    "        kappa_scores_data.setdefault(finding, {}).update({f'Cohen\\'s Kappa {model_name.title()}': score})\n",
    "    kappa_scores_data.setdefault('Average Kappa', {}).update({f'Cohen\\'s Kappa {model_name.title()}': average_kappa})\n",
    "\n",
    "# Converting the kappa_scores_data dictionary to a DataFrame and rounding the scores for better readability.\n",
    "kappa_scores_df = pd.DataFrame(kappa_scores_data).T.round(3)\n",
    "\n",
    "# Order the columns to make it easier to transfer to overleaf\n",
    "ordered_columns = [\n",
    "    'Cohen\\'s Kappa Chexpert', 'Cohen\\'s Kappa Chexbert', 'Cohen\\'s Kappa Mistral7B', \n",
    "    'Cohen\\'s Kappa Llama13B', 'Cohen\\'s Kappa Mixtral8X7B', 'Cohen\\'s Kappa Llama70B',\n",
    "    'Cohen\\'s Kappa Qwen_72B', 'Cohen\\'s Kappa Gpt_35', 'Cohen\\'s Kappa Gpt_4',\n",
    "]\n",
    "kappa_scores_df = kappa_scores_df.reindex(columns=ordered_columns)\n",
    "\n",
    "kappa_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save kappa scores\n",
    "kappa_scores_df.to_csv(os.path.join(base_path, 'kappa_scores_zero_shot_mgb.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd76f8",
   "metadata": {},
   "source": [
    "## Label Frequency in Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count values for each column\n",
    "ground_truth_count = ground_truth.apply(pd.Series.value_counts)\n",
    "ground_truth_count = ground_truth_count.T.fillna(0).astype(int)\n",
    "\n",
    "#ground_truth_count.to_csv('ground_truth_count.csv')\n",
    "ground_truth_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738bbfc7",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test_corrected(model1, model2, diseases, ground_truth):\n",
    "    valid_values = {\"Yes\", \"No\", \"Maybe\"}\n",
    "    \n",
    "    def check_values(df, columns):\n",
    "        for col in columns:\n",
    "            if not df[col].isin(valid_values).all():\n",
    "                raise ValueError(f\"Column {col} contains values other than 'Yes', 'Maybe' or 'No'.\")\n",
    "    \n",
    "    check_values(model1, diseases)\n",
    "    check_values(model2, diseases)\n",
    "\n",
    "    results = []\n",
    "    combined_contingency = [0, 0, 0, 0]  # yes_yes, yes_no, no_yes, no_no\n",
    "    \n",
    "    for disease in diseases:\n",
    "        correct_correct = ((model1[disease] == ground_truth[disease]) & (model2[disease] == ground_truth[disease])).sum()\n",
    "        correct_incorrect = ((model1[disease] == ground_truth[disease]) & (model2[disease] != ground_truth[disease])).sum()\n",
    "        incorrect_correct = ((model1[disease] != ground_truth[disease]) & (model2[disease] == ground_truth[disease])).sum()\n",
    "        incorrect_incorrect = ((model1[disease] != ground_truth[disease]) & (model2[disease] != ground_truth[disease])).sum()\n",
    "        \n",
    "        combined_contingency[0] += correct_correct\n",
    "        combined_contingency[1] += correct_incorrect\n",
    "        combined_contingency[2] += incorrect_correct\n",
    "        combined_contingency[3] += incorrect_incorrect\n",
    "        \n",
    "        contingency_table = [[correct_correct, correct_incorrect], [incorrect_correct, incorrect_incorrect]]\n",
    "\n",
    "        result = mcnemar(contingency_table, exact=True if min(correct_incorrect, incorrect_correct) < 25 else False)\n",
    "        stat = result.statistic\n",
    "        p = result.pvalue\n",
    "        \n",
    "        results.append({\n",
    "            \"Disease\": disease,\n",
    "            \"McNemar Statistic\": stat,\n",
    "            \"P-value\": p\n",
    "        })\n",
    "    \n",
    "    # Perform McNemar test on the combined contingency table\n",
    "    combined_table = [[combined_contingency[0], combined_contingency[1]],\n",
    "                      [combined_contingency[2], combined_contingency[3]]]\n",
    "    print(combined_table)\n",
    "    combined_result = mcnemar(combined_table, exact=True if np.sum([combined_contingency[1], combined_contingency[2]])<25 else False)\n",
    "    # Append combined results\n",
    "    results.append({\n",
    "        \"Disease\": \"All Combined\",\n",
    "        \"McNemar Statistic\": combined_result.statistic,\n",
    "        \"P-value\": combined_result.pvalue\n",
    "    })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    p_values = results_df[\"P-value\"].values\n",
    "    bonferroni_corrected = multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "    results_df['Bonferroni-corrected P-value'] = bonferroni_corrected[1]\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "aggregated_results_mcnemar = pd.DataFrame()\n",
    "\n",
    "for model_name, model_predictions in predictions.items():\n",
    "\n",
    "    results_df_mcnemar = mcnemar_test_corrected(gpt_4, model_predictions, diseases, y_true)\n",
    "    results_df_mcnemar['Model'] = model_name  # Add a column for the model name\n",
    "    # Ensure the 'Disease' column is included before concatenation\n",
    "    aggregated_results_mcnemar = pd.concat([aggregated_results_mcnemar, results_df_mcnemar], axis=0)\n",
    "\n",
    "\n",
    "# Resetting index for better readability and to avoid duplicate indices\n",
    "aggregated_results_mcnemar.reset_index(drop=True, inplace=True)\n",
    "aggregated_results_mcnemar.set_index([\"Model\"], inplace=True)\n",
    "total_mcnemar = aggregated_results_mcnemar[aggregated_results_mcnemar[\"Disease\"] == \"All Combined\"]\n",
    "total_mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21310d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mcnemar.to_csv(os.path.join(base_path, 'total_mcnemar_few_shot_positive_binary.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83611c5",
   "metadata": {},
   "source": [
    "## Radar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c798df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set findings\n",
    "findings = f1_scores_df.index[:13] \n",
    "\n",
    "# replace _ with sspace in findings for nicer plotting\n",
    "findings = [finding.replace(\"_\", \" \") for finding in findings.tolist()]\n",
    "\n",
    "# Results for the revlevant models\n",
    "values_gpt4 = f1_scores_df['F1 Score Gpt_4'][:13].tolist()\n",
    "values_qwen = f1_scores_df['F1 Score Qwen_72B'][:13].tolist()\n",
    "values_mixtral = f1_scores_df['F1 Score Mixtral8X7B'][:13].tolist()\n",
    "values_llama = f1_scores_df['F1 Score Llama70B'][:13].tolist()\n",
    "\n",
    "# Complete the loop for the radar chart\n",
    "values_gpt4 += values_gpt4[:1]\n",
    "values_qwen += values_qwen[:1]\n",
    "values_mixtral += values_mixtral[:1]\n",
    "values_llama += values_llama[:1]\n",
    "\n",
    "num_vars = len(findings)\n",
    "\n",
    "# Split the circle into even parts and save the angles\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# Complete the loop for the radar chart\n",
    "angles += angles[:1]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "plt.xticks(angles[:-1], findings, color='black', size=8)\n",
    "\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks(color=\"grey\", size=7)\n",
    "plt.ylim(0.5, 1)\n",
    "\n",
    "ax.plot(angles, values_gpt4, linewidth=1, linestyle='solid', label='GPT-4')\n",
    "ax.plot(angles, values_qwen, linewidth=1, linestyle='solid', label='QWEN1.5-72B')\n",
    "ax.plot(angles, values_mixtral, linewidth=1, linestyle='solid', label='Mixtral-8x7B')\n",
    "ax.plot(angles, values_llama, linewidth=1, linestyle='solid', label='Llama2-70B')\n",
    "\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "plt.savefig(os.path.join(base_path, 'radar_plot_few_shot_positive_binary.svg'), format='svg', bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray-drift-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
